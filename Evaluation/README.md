# Evaluation Frameworks for Unlearning

Welcome! This repository collects various frameworks for sampling and evaluating unlearning methods. If you're working on machine unlearning, you'll find useful benchmarks and tools here.

## UnlearnCanvas Benchmark

In our paper, we utilize the **UnlearnCanvas** benchmark (NeurIPS 2024) to construct a sequence of 12 styles and objects to unlearn. The sampling and evaluation procedures are adapted from the [UnlearnCanvas repository](https://github.com/OPTML-Group/UnlearnCanvas), with minor modifications for our experiments.

## How to Use

1. **Explore Frameworks:**  
    Browse the available sampling and evaluation frameworks in this directory.

2. **Reproduce Results:**  
    Follow the instructions in each subfolder to reproduce our experiments or adapt them for your own research.

3. **Contribute:**  
    Contributions are welcome! Feel free to add new frameworks or improvements via pull requests.

## References

- [UnlearnCanvas GitHub Repository](https://github.com/OPTML-Group/UnlearnCanvas)
---

For questions or suggestions, please open an issue or contact the maintainers.